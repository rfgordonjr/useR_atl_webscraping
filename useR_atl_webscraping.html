<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>An Introduction to Web Scraping in R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Rob Gordon" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# An Introduction to Web Scraping in R
### Rob Gordon
### Jan 21, 2018 (updated: 2021-01-11)

---




# About Me

.pull-left[
- Using R since 2008 (courtesy of [University of Florida's Department of Statistics](https://stat.ufl.edu))
- Lead Data Scientist at [Stanley Black &amp; Decker](https://www.stanleyblackanddecker.com)
- Social Media
    - &lt;svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 448 512"&gt;&lt;path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/&gt;&lt;/svg&gt; [robert-gordon-35090740](https://www.linkedin.com/in/robert-gordon-35090740/)
    - &lt;svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; [rfgordonjr](https://github.com/rfgordonjr)
    - &lt;svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 448 512"&gt;&lt;path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/&gt;&lt;/svg&gt; [@flashg03](https://www.instagram.com/flashg03/)
]

.pull-right[
.center[
![](images/University_of_Florida_logo.svg)

![](images/Stanley_Black_&amp;_Decker_logo.svg)

![](images/layla.jpg)
]
]

---

# Motivation: Focused Laziness

![](images/laziness.png)


Laziness is indistinguishable from efficiency when you work towards automation rather than repetitive tasks.

[Never copy/paste if you don't have to.](https://portswigger.net/daily-swig/underestimated-dangers-of-copy-and-paste-exposed)


???

I'd rather dedicate work or energy to automating a task (pulling data from the web) with the press of a button than navigating to a site once a month, and copying and pasting data, esp since I know the website information will change over time. 

---

# Introducing Web Scraping but also...

I'll spend plenty of time about what web scraping is and how to do it in different situations.

But this is also a talk about *not* web scraping. I'll start by talking about:

- Other options to consider first.
- Why you shouldn't do it.
- How to check if its allowed.

*Then* i'll talk about what it is and how to do it.

---

# Methods to consider before scraping:

Have you tried:

- Direct download (base)
- APIs

---

# Direct Download

This might be the best-case scenario. 

Copy/paste the url of the data's location to a function in R.


```r
## From Casella's Statistical Design:
corn_dat &lt;- read.table(file = "http://archived.stat.ufl.edu/casella/StatDesign/WebDataSets/Corn.txt", header = TRUE)
head(corn_dat)
```

```
##   Trt  X    Y
## 1 M-4 20 12.8
## 2 M-4 17 11.0
## 3 M-4 20 10.9
## 4 M-4 15  9.1
## 5 M-4 20  9.6
## 6 M-4 15  9.3
```

---

# APIs

[API](https://medium.freecodecamp.org/what-is-an-api-in-english-please-b880a3214a82) stands for Application Programming Interface. 

APIs typically require setting up an account and generating a key to access data.

Generally APIs

- are easy to use.
- standardize the ways customers and employees interact with data. 

The website owner creates a very specific way for users to pull information from their site.

- Best case scenario: company makes an API you can interact with via a dedicated R packages.
- Less good scenario: build your own [RESTful](https://www.tutorialspoint.com/restful/) API.

---

# Last Resort: Web Scraping

Before describing how this works everyone should first be aware that

- it may violate the site's Terms of Service
- it can cause an unexpected load on the site.

IANAL, but there are many grey areas for legality of scraping websites.

.center[
[__Yes__](https://en.wikipedia.org/wiki/Kelly_v._Arriba_Soft_Corp.#Fair_use_analysis) &amp;nbsp; &amp;nbsp; &amp;nbsp; [__you__](https://en.wikipedia.org/wiki/Associated_Press_v._Meltwater_U.S._Holdings,_Inc.) &amp;nbsp; &amp;nbsp; &amp;nbsp; [__can__](https://petewarden.com/2010/04/05/how-i-got-sued-by-facebook/) &amp;nbsp; &amp;nbsp; &amp;nbsp; [__get__](http://michaelgkeating.com/cant-find-me-on-linkedin-heres-why-i-got-kicked-off/) &amp;nbsp; &amp;nbsp; &amp;nbsp; [__sued__](https://www.docketalarm.com/cases/California_Northern_District_Court/5--14-cv-00068/LinkedIn_Corporation_v._Robocog_Inc/docs/8.pdf).
]

At minimum you should do the following before attempting to scrape a site:

- Read the Terms of Service
- Check for the presence of a `robots.txt` file, i.e. a set of instructions for web robots.

---

# Quick Facts about `robots.txt`

`robots.txt` is typically a single text file informing web scrapers

- which robots are allowed to access the site
- which pages are disallowed to robots

See (http://www.robotstxt.org/robotstxt.html) for full details.

We will review the basics and some examples.

---

# `robots.txt` examples (1/2)

A site with this content in robots.txt disallows all robots from all pages of the site:

```
User-agent: *
Disallow: /
```

Here all robots are banned from 3 specific directories:

```
User-agent: *
Disallow: /cgi-bin/
Disallow: /tmp/
Disallow: /~joe/
```

Here all robots are allowed complete access to the entire site:

```
User-agent: *
Disallow:
```

---

# `robots.txt` examples (2/2)

Here a single robot is excluded:

```
User-agent: BadBot
Disallow: /
```

Here specific pages are disallowed:

```
User-agent: *
Disallow: /~joe/junk.html
Disallow: /~joe/foo.html
Disallow: /~joe/bar.html
```

---

# Where can we find `robots.txt`?

Typically it lives in the root of the website.

For example, if you want to find the `robots.txt` file for 

`https://en.wikipedia.org` 

just type:

`https://en.wikipedia.org/robots.txt`

R lets us build the checking of the `robots.txt` file into our workflow.


---

# Survey of [polite](https://github.com/dmi3kno/polite)

.pull-left[
Lucky for us the [polite](https://github.com/dmi3kno/polite) package exists to check permissions from the `robots.txt` file.


.center[
![](images/polite_logo.png)
]

]

.pull-right[
I highly encourage anyone interested in web scraping to visit the github site but the main goals of the package is to encourage

- seeking permission
- pull data slowly
- never asking twice
 
]

---

# Simple `polite` example (not scrapable)

`polite` has 3 main functions worth knowing:

- `bow`: read permissions from `robots.txt`
- `scrape`: pull content of site via `bow` object
- `nod`: use to adjust a scraping url, making sure the new location is allowed by `robots.txt`


```r
library(polite)
sessionLinkedIn &lt;- bow("https://www.linkedin.com", force = TRUE)
sessionLinkedIn
```

```
## &lt;polite session&gt; https://www.linkedin.com
##     User-agent: polite R package - https://github.com/dmi3kno/polite
##     robots.txt: 2987 rules are defined for 35 bots
##    Crawl delay: 5 sec
##   The path is not scrapable for this user-agent
```

---

# Simple `polite` example (scrapable)

Here we see for wikipedia we are allowed to scrape the site.


```r
sessionWiki &lt;- bow("https://www.wikipedia.org", force = TRUE)
sessionWiki
```

```
## &lt;polite session&gt; https://www.wikipedia.org
##     User-agent: polite R package - https://github.com/dmi3kno/polite
##     robots.txt: 456 rules are defined for 33 bots
##    Crawl delay: 5 sec
##   The path is scrapable for this user-agent
```

Before we demonstrate `polite::scrape` we'll discuss how the `rvest` package works, as we'll be using that in concert with `polite`.

---

# Last `polite` example: Nike has fun


```r
sessionNike &lt;- bow("https://www.nike.com/us/en_us/", force = TRUE)
sessionNike$robotstxt$text %&gt;% substr(start = 1, stop = 42)
```

```
## [robots.txt]
## --------------------------------------
## 
## # www.nike.com robots.txt -- just crawl it
```

End of robots.txt:

![](images/nikeSwoosh_robots_txt.png)

---

# Create a TRUE/FALSE variable for scraping

1. Use the `polite` package to `bow` and ask for permission to scrape.
2. Create a function called to pull `robots.txt` file in a nice list.
3. Write another function to create a boolean TRUE/FALSE variable to build into our workflow a check before scraping.

Function definitions in appendix slides or at the source: &lt;https://www.ddrive.no/post/be-nice-on-the-web/&gt;



---

## Apply to our examples: LinkedIn


```r
sessionLinkedIn$url
```

```
## [1] "https://www.linkedin.com"
```

```r
sessionLinkedIn$user_agent
```

```
## [1] "polite R package - https://github.com/dmi3kno/polite"
```

```r
check_rtxt(url = sessionLinkedIn$url, 
           delay = 5, 
           user_agent = sessionLinkedIn$user_agent,
           force = FALSE,
           verbose = FALSE)
```

```
## Warning in check_rtxt(url = sessionLinkedIn$url, delay = 5, user_agent =
## sessionLinkedIn$user_agent, : robots.txt says this path is NOT scrapable for
## your user agent!
```

```
## [1] FALSE
```

---

## Apply to our examples: Wikipedia


```r
sessionWiki$url
```

```
## [1] "https://www.wikipedia.org"
```

```r
sessionWiki$user_agent
```

```
## [1] "polite R package - https://github.com/dmi3kno/polite"
```

```r
check_rtxt(url = sessionWiki$url, 
           delay = 5, 
           user_agent = sessionWiki$user_agent,
           force = FALSE,
           verbose = FALSE)
```

```
## [1] TRUE
```

---

## Apply to our examples: Nike


```r
sessionNike$url
```

```
## [1] "https://www.nike.com/us/en_us/"
```

```r
sessionNike$user_agent
```

```
## [1] "polite R package - https://github.com/dmi3kno/polite"
```

```r
check_rtxt(url = sessionNike$url, 
           delay = 5, 
           user_agent = sessionNike$user_agent,
           force = FALSE,
           verbose = FALSE)
```

```
## [1] TRUE
```

---

# Example 1: Scraping Wikipedia

Grab list of most visited country wikipedia pages (December 1, 2007 – January 1, 2020).

From &lt;https://www.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages&gt;:

.center[
![](images/screenShotWikiTableCountries.png)
]

---

# Example 1: Use nod to modify the url

Recall we already established permission with the `bow` function. 

Since we are visiting a modified part of Wikipedia use the `nod` function to navigate to the new path.

This updates the session’s URL, making sure that the new location can be negotiated against `robots.txt`.


```r
library(rvest)
url &lt;- "https://www.wikipedia.org"
newSession &lt;- bow(url) %&gt;% 
  nod(path = "wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages")
newSession
```

```
## &lt;polite session&gt; https://www.wikipedia.org/wiki/Wikipedia:Multiyear_ranking_of_most_viewed_pages
##     User-agent: polite R package - https://github.com/dmi3kno/polite
##     robots.txt: 456 rules are defined for 33 bots
##    Crawl delay: 5 sec
##   The path is scrapable for this user-agent
```

---

# Optional: Create the boolean condition

Then use our function to create a boolean variable:


```r
hasPermission &lt;- check_rtxt(url = newSession$url, 
                            delay = 5, 
                            user_agent = newSession$user_agent,
                            force = FALSE,
                            verbose = FALSE)
hasPermission
```

```
## [1] TRUE
```

---

# Example 1: scrape with `rvest`

If we have permission, go ahead and scrape the table into memory:


```r
if(hasPermission){
  listWikiTables &lt;- scrape(newSession) %&gt;% 
    html_nodes(xpath = '//*[@class="wikitable"]') %&gt;% 
    html_table()
  listWikiTables[[2]] %&gt;% head()
} else {
  paste("No permission.")
}
```

```
##   Rank           Page Views in millions
## 1    1  United States               204
## 2    2          India               128
## 3    3 United Kingdom               114
## 4    4         Canada                92
## 5    5      Australia                87
## 6    6          China                79
```

---

# How did we get that?

.pull-left[
![](images/rvest.png)
]
.pull-right[
Use clues from the Document Object Model (DOM)

- [Selector Gadget](https://selectorgadget.com)
- Browser Developer Tools

Live Demo with Chrome
]

---

class: inverse, center, middle

background-image: url("images/DOM.png")
background-position: center
background-size: contain

# Introducing the DOM


---

# What if we have a dynamic webpage?

What do we mean by dynamic?

Situations where we have BOTH:

- Content generated by interacting with UI.
- Changing url string does nothing to update the data.

---

# Motivating Example

Employee spending at SBD hotels.

- Study where employees stay.
- Cluster hotel locations.
- Find hotel stays far from preferred hotels.
- Plot information on a map.

Problems: 
- Updated hotel list is lives at intranet site.
- List changes often.
- List generated dynamically from site UI.
- The data changes, but the url stays the same.

---

# Use bots to pull dynamically generated data


One way to do this is via a Selenium server...

.center[
![](images/Seleniumlogo.png)
]

with a Docker container.
.center[
![](images/docker_vertical.png)
]

---

# What are these?

[Selenium](https://en.wikipedia.org/wiki/Selenium_(software) is a portable framework for testing web applications.

A [Docker](https://www.docker.com/resources/what-container) container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.

Think of this as similar to virtual machines (VMs) but differ in an important way; containers virtualize the OS instead of the hardware.

---

# Quick tangent: Docker + Selenium does more than just web scraping

&gt; Selenium is a portable software-testing framework for web applications.

Selenium can be used for *any* software related to web deployment. Some other R-specific examples include:

1. [Persistent reproducible reporting](https://cran.r-project.org/web/packages/liftr/vignettes/liftr-intro.html)
2. [Testing](http://rpubs.com/johndharrison/13408) and [deploying shiny apps](https://www.shinyproxy.io).

These topics are outside the scope of this talk but worth noting as relevant subjects for an R user group.

---

# Final tangent: Is it *really* dynamic?

Many website generate content dynamically (i.e. according to user input). That doesn't mean we are forced to use Selenium.

We only need Selenium if interaction with the website is absolutely necessary to generate its content.

Do this one last check to see if you can avoid using Selenium: 

1. Check if interacting with the website creates a url that lets you recover the web page.
2. If copying/pasting the resulting url reproduces the website's content, you can manipulate the url string to suit your needs.

---

# Step 1: Install Docker

---

# Step 2: Start a selenium server on the docker container

---

# Step 3: Call a web browser inside the selenium server using `RSelenium`

---

# Step 4: Navigate to the url and interact with the site

---

# Step 5: Find the DOM and use `rvest` as before

---

class: center, inverse, middle

# Appendix

---

# Pull `robots.txt` into a nice list.

First build a helper function:

&lt;style type="text/css"&gt;
.scroll-box-8 {
  height:8em;
  overflow-y: scroll;
}
.scroll-box-10 {
  height:10em;
  overflow-y: scroll;
}
.scroll-box-12 {
  height:12em;
  overflow-y: scroll;
}
.scroll-box-14 {
  height:14em;
  overflow-y: scroll;
}
.scroll-box-16 {
  height:16em;
  overflow-y: scroll;
}
.scroll-box-18 {
  height:18em;
  overflow-y: scroll;
}
.scroll-box-20 {
  height:20em;
  overflow-y: scroll;
}
.scroll-output {
  height: 90%;
  overflow-y: scroll;
}
.scroll-100 {
  max-height: 50vh;
  overflow-y: auto;
  background-color: inherit;
}
&lt;/style&gt;

.scroll-100[

```r
library(robotstxt)
library(ratelimitr)
library(memoise)
#' generated by polite::use_manners()
#' null-coalescing operator. See purr for details.
`%||%` &lt;- function(lhs, rhs) {
  if (!is.null(lhs) &amp;&amp; length(lhs) &gt; 0) lhs else rhs
}
```
]

---

# Get `robots.txt` info



```r
#' generated by polite::use_manners()
#' function to get robots.txt is structured form. Memoised
polite_fetch_rtxt &lt;- memoise::memoise(
  function(..., user_agent, delay, verbose){
  rt &lt;- robotstxt::robotstxt(...)
  delay_df &lt;- rt$crawl_delay
  crawldelays &lt;- as.numeric(
    delay_df[with(delay_df,useragent==user_agent),"value"]) %||%
    as.numeric(delay_df[with(delay_df, useragent=="*"), "value"]) %||% 0
  rt$delay_rate &lt;- max(crawldelays, delay, 1)
  if(verbose){
    message("Bowing to: ", rt$domain)
    message("There's ",nrow(delay_df),
            " crawl delay rule(s) defined for this host.")
    message("Your rate will be set to 1 request every ",
            rt$delay_rate," second(s).")}
  rt
})
```


---

# Create a boolean TRUE/FALSE variable.


```r
#' generated by polite::use_manners()
#' function to check url against robots.txt and enforce appropriate delay
check_rtxt &lt;-function(url, delay, user_agent, force, verbose){
  url_parsed &lt;- httr::parse_url(url)
  host_url &lt;- paste0(url_parsed$scheme, "://", url_parsed$hostname)
  rt &lt;- polite_fetch_rtxt(host_url, 
                          force=force, 
                          user_agent=user_agent,
                          delay=delay, 
                          verbose=verbose)
  is_scrapable &lt;- rt$check(paths=url_parsed$path, 
                           bot=user_agent)

  if(is_scrapable)
    Sys.sleep(rt$delay_rate)
  else
    warning("robots.txt says this path is NOT scrapable for your user agent!")

  is_scrapable
}
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
